{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del Modelo y del Video a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo preentrenado\n",
    "model = YOLO('yolov8m.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui se abre el video \n",
    "cap = cv.VideoCapture('V_Mall.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraciones inciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se configura la resolución del video capturado\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 1024)  # Aquí establecemos el ancho \n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 576)  # Aquí establecemos el alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el umbral de confianza para las detecciones\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para poder rastrear posiciones y valores persistentes\n",
    "previous_positions = {}\n",
    "frame_count = 0\n",
    "person_count = 0\n",
    "density = 0\n",
    "moving_ratio = 0\n",
    "occupied_area_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del VideoWriter\n",
    "frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "\n",
    "output_filename = 'video_resultado.avi'\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')  # Codec para AVI\n",
    "out = cv.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento del Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 1 handbag, 576.8ms\n",
      "Speed: 18.6ms preprocess, 576.8ms inference, 32.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 354.4ms\n",
      "Speed: 27.9ms preprocess, 354.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 411.4ms\n",
      "Speed: 2.4ms preprocess, 411.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 5 handbags, 370.7ms\n",
      "Speed: 3.0ms preprocess, 370.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 550.0ms\n",
      "Speed: 3.0ms preprocess, 550.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 568.8ms\n",
      "Speed: 4.5ms preprocess, 568.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 627.9ms\n",
      "Speed: 7.0ms preprocess, 627.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 538.0ms\n",
      "Speed: 8.1ms preprocess, 538.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 529.7ms\n",
      "Speed: 4.2ms preprocess, 529.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 540.0ms\n",
      "Speed: 6.0ms preprocess, 540.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 543.1ms\n",
      "Speed: 3.9ms preprocess, 543.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 570.7ms\n",
      "Speed: 4.6ms preprocess, 570.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 594.9ms\n",
      "Speed: 4.0ms preprocess, 594.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 532.5ms\n",
      "Speed: 7.5ms preprocess, 532.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 567.1ms\n",
      "Speed: 4.5ms preprocess, 567.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 589.0ms\n",
      "Speed: 6.6ms preprocess, 589.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 740.2ms\n",
      "Speed: 11.4ms preprocess, 740.2ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 725.7ms\n",
      "Speed: 7.5ms preprocess, 725.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 844.3ms\n",
      "Speed: 10.6ms preprocess, 844.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 831.6ms\n",
      "Speed: 9.5ms preprocess, 831.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 1060.4ms\n",
      "Speed: 7.0ms preprocess, 1060.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 1029.8ms\n",
      "Speed: 8.0ms preprocess, 1029.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 994.6ms\n",
      "Speed: 13.1ms preprocess, 994.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 1015.6ms\n",
      "Speed: 12.6ms preprocess, 1015.6ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 953.9ms\n",
      "Speed: 12.2ms preprocess, 953.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 1097.5ms\n",
      "Speed: 11.6ms preprocess, 1097.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 621.5ms\n",
      "Speed: 4.6ms preprocess, 621.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 584.5ms\n",
      "Speed: 5.0ms preprocess, 584.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 556.1ms\n",
      "Speed: 3.5ms preprocess, 556.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 548.4ms\n",
      "Speed: 8.0ms preprocess, 548.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 595.8ms\n",
      "Speed: 8.3ms preprocess, 595.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 644.9ms\n",
      "Speed: 9.5ms preprocess, 644.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 645.1ms\n",
      "Speed: 9.7ms preprocess, 645.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 582.1ms\n",
      "Speed: 3.0ms preprocess, 582.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 535.5ms\n",
      "Speed: 8.3ms preprocess, 535.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 580.8ms\n",
      "Speed: 4.0ms preprocess, 580.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 542.7ms\n",
      "Speed: 6.6ms preprocess, 542.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 535.9ms\n",
      "Speed: 3.5ms preprocess, 535.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 529.5ms\n",
      "Speed: 7.7ms preprocess, 529.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 528.2ms\n",
      "Speed: 3.5ms preprocess, 528.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 518.8ms\n",
      "Speed: 4.5ms preprocess, 518.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 533.3ms\n",
      "Speed: 7.7ms preprocess, 533.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 528.1ms\n",
      "Speed: 9.3ms preprocess, 528.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 531.9ms\n",
      "Speed: 5.0ms preprocess, 531.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 581.1ms\n",
      "Speed: 5.1ms preprocess, 581.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 554.6ms\n",
      "Speed: 6.0ms preprocess, 554.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 620.0ms\n",
      "Speed: 6.1ms preprocess, 620.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 527.8ms\n",
      "Speed: 3.6ms preprocess, 527.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 546.6ms\n",
      "Speed: 4.0ms preprocess, 546.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 528.6ms\n",
      "Speed: 2.0ms preprocess, 528.6ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 538.4ms\n",
      "Speed: 4.0ms preprocess, 538.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 520.6ms\n",
      "Speed: 8.1ms preprocess, 520.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 524.6ms\n",
      "Speed: 5.0ms preprocess, 524.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 536.3ms\n",
      "Speed: 3.5ms preprocess, 536.3ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 529.4ms\n",
      "Speed: 5.0ms preprocess, 529.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 541.6ms\n",
      "Speed: 3.5ms preprocess, 541.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 533.1ms\n",
      "Speed: 5.5ms preprocess, 533.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 537.6ms\n",
      "Speed: 3.0ms preprocess, 537.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 493.8ms\n",
      "Speed: 4.6ms preprocess, 493.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 531.0ms\n",
      "Speed: 2.5ms preprocess, 531.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 552.5ms\n",
      "Speed: 2.5ms preprocess, 552.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 549.1ms\n",
      "Speed: 3.5ms preprocess, 549.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 handbags, 613.0ms\n",
      "Speed: 4.5ms preprocess, 613.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 565.4ms\n",
      "Speed: 4.1ms preprocess, 565.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 handbag, 551.8ms\n",
      "Speed: 8.5ms preprocess, 551.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 545.1ms\n",
      "Speed: 7.0ms preprocess, 545.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 548.1ms\n",
      "Speed: 4.0ms preprocess, 548.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 546.9ms\n",
      "Speed: 3.0ms preprocess, 546.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 571.9ms\n",
      "Speed: 8.1ms preprocess, 571.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 589.1ms\n",
      "Speed: 4.0ms preprocess, 589.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 backpack, 3 handbags, 568.6ms\n",
      "Speed: 7.7ms preprocess, 568.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 560.3ms\n",
      "Speed: 6.0ms preprocess, 560.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 571.2ms\n",
      "Speed: 10.1ms preprocess, 571.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 532.9ms\n",
      "Speed: 2.6ms preprocess, 532.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 542.9ms\n",
      "Speed: 4.5ms preprocess, 542.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 549.6ms\n",
      "Speed: 8.2ms preprocess, 549.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 570.5ms\n",
      "Speed: 9.5ms preprocess, 570.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 565.8ms\n",
      "Speed: 3.0ms preprocess, 565.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 566.0ms\n",
      "Speed: 3.0ms preprocess, 566.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 601.4ms\n",
      "Speed: 8.6ms preprocess, 601.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 604.3ms\n",
      "Speed: 7.6ms preprocess, 604.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 563.5ms\n",
      "Speed: 9.6ms preprocess, 563.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 596.0ms\n",
      "Speed: 5.4ms preprocess, 596.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 597.5ms\n",
      "Speed: 7.6ms preprocess, 597.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 4 handbags, 555.5ms\n",
      "Speed: 10.6ms preprocess, 555.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 605.4ms\n",
      "Speed: 6.7ms preprocess, 605.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 574.3ms\n",
      "Speed: 8.6ms preprocess, 574.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 606.6ms\n",
      "Speed: 8.1ms preprocess, 606.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 571.3ms\n",
      "Speed: 7.0ms preprocess, 571.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 589.0ms\n",
      "Speed: 4.1ms preprocess, 589.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 561.4ms\n",
      "Speed: 3.0ms preprocess, 561.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 560.8ms\n",
      "Speed: 8.0ms preprocess, 560.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 538.5ms\n",
      "Speed: 5.0ms preprocess, 538.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 593.3ms\n",
      "Speed: 3.5ms preprocess, 593.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 555.9ms\n",
      "Speed: 8.6ms preprocess, 555.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 581.7ms\n",
      "Speed: 5.7ms preprocess, 581.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 handbag, 558.3ms\n",
      "Speed: 5.6ms preprocess, 558.3ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 handbags, 550.2ms\n",
      "Speed: 3.6ms preprocess, 550.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 541.1ms\n",
      "Speed: 6.0ms preprocess, 541.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 547.6ms\n",
      "Speed: 6.9ms preprocess, 547.6ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 580.5ms\n",
      "Speed: 3.0ms preprocess, 580.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 526.1ms\n",
      "Speed: 9.5ms preprocess, 526.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 555.0ms\n",
      "Speed: 3.0ms preprocess, 555.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 555.2ms\n",
      "Speed: 3.0ms preprocess, 555.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 handbag, 572.1ms\n",
      "Speed: 9.2ms preprocess, 572.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 638.8ms\n",
      "Speed: 4.0ms preprocess, 638.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 522.0ms\n",
      "Speed: 8.5ms preprocess, 522.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 546.8ms\n",
      "Speed: 3.1ms preprocess, 546.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 439.9ms\n",
      "Speed: 7.2ms preprocess, 439.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 569.8ms\n",
      "Speed: 4.0ms preprocess, 569.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 550.9ms\n",
      "Speed: 3.0ms preprocess, 550.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 573.9ms\n",
      "Speed: 4.0ms preprocess, 573.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 558.7ms\n",
      "Speed: 9.6ms preprocess, 558.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5 handbags, 553.8ms\n",
      "Speed: 4.0ms preprocess, 553.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 526.4ms\n",
      "Speed: 10.0ms preprocess, 526.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 handbags, 564.5ms\n",
      "Speed: 4.6ms preprocess, 564.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 548.0ms\n",
      "Speed: 7.0ms preprocess, 548.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 528.0ms\n",
      "Speed: 4.0ms preprocess, 528.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 backpack, 2 handbags, 546.9ms\n",
      "Speed: 6.0ms preprocess, 546.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 564.8ms\n",
      "Speed: 5.0ms preprocess, 564.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 540.5ms\n",
      "Speed: 7.6ms preprocess, 540.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 570.3ms\n",
      "Speed: 4.0ms preprocess, 570.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 562.4ms\n",
      "Speed: 3.6ms preprocess, 562.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 535.9ms\n",
      "Speed: 8.6ms preprocess, 535.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 540.8ms\n",
      "Speed: 9.1ms preprocess, 540.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 558.5ms\n",
      "Speed: 9.7ms preprocess, 558.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 551.4ms\n",
      "Speed: 3.5ms preprocess, 551.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 566.3ms\n",
      "Speed: 4.6ms preprocess, 566.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 569.1ms\n",
      "Speed: 5.0ms preprocess, 569.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 570.0ms\n",
      "Speed: 9.6ms preprocess, 570.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 600.0ms\n",
      "Speed: 3.3ms preprocess, 600.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 563.0ms\n",
      "Speed: 4.1ms preprocess, 563.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 598.4ms\n",
      "Speed: 4.0ms preprocess, 598.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 529.6ms\n",
      "Speed: 4.6ms preprocess, 529.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 581.6ms\n",
      "Speed: 5.5ms preprocess, 581.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 605.2ms\n",
      "Speed: 6.0ms preprocess, 605.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 586.2ms\n",
      "Speed: 10.1ms preprocess, 586.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 562.7ms\n",
      "Speed: 4.0ms preprocess, 562.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 562.8ms\n",
      "Speed: 5.6ms preprocess, 562.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 584.6ms\n",
      "Speed: 5.0ms preprocess, 584.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 593.8ms\n",
      "Speed: 5.6ms preprocess, 593.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 573.2ms\n",
      "Speed: 9.5ms preprocess, 573.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 5 handbags, 572.0ms\n",
      "Speed: 8.5ms preprocess, 572.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 572.6ms\n",
      "Speed: 5.0ms preprocess, 572.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 backpack, 3 handbags, 574.5ms\n",
      "Speed: 5.9ms preprocess, 574.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 616.2ms\n",
      "Speed: 3.0ms preprocess, 616.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 backpack, 3 handbags, 664.3ms\n",
      "Speed: 4.0ms preprocess, 664.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 3 handbags, 647.9ms\n",
      "Speed: 5.0ms preprocess, 647.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 675.4ms\n",
      "Speed: 9.6ms preprocess, 675.4ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 616.9ms\n",
      "Speed: 10.9ms preprocess, 616.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 590.4ms\n",
      "Speed: 9.6ms preprocess, 590.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 596.9ms\n",
      "Speed: 4.1ms preprocess, 596.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 579.1ms\n",
      "Speed: 9.7ms preprocess, 579.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 608.5ms\n",
      "Speed: 4.0ms preprocess, 608.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 617.7ms\n",
      "Speed: 10.2ms preprocess, 617.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 586.2ms\n",
      "Speed: 3.0ms preprocess, 586.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 5 handbags, 510.5ms\n",
      "Speed: 8.5ms preprocess, 510.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 492.5ms\n",
      "Speed: 3.0ms preprocess, 492.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 539.5ms\n",
      "Speed: 8.5ms preprocess, 539.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 741.5ms\n",
      "Speed: 25.6ms preprocess, 741.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 676.0ms\n",
      "Speed: 4.5ms preprocess, 676.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 5 handbags, 667.9ms\n",
      "Speed: 7.5ms preprocess, 667.9ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 587.7ms\n",
      "Speed: 9.6ms preprocess, 587.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 552.5ms\n",
      "Speed: 8.5ms preprocess, 552.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 529.7ms\n",
      "Speed: 5.5ms preprocess, 529.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 537.9ms\n",
      "Speed: 3.5ms preprocess, 537.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 538.1ms\n",
      "Speed: 9.7ms preprocess, 538.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 575.1ms\n",
      "Speed: 3.5ms preprocess, 575.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 553.9ms\n",
      "Speed: 5.0ms preprocess, 553.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Procesar video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Fin del video.\")\n",
    "        break\n",
    "\n",
    "    # Realizar detección de objetos con YOLOv8\n",
    "    results = model(frame)\n",
    "\n",
    "    # Obtener las detecciones en formato xyxy\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    # Variables para conteo y cálculo de densidad\n",
    "    current_person_count = 0\n",
    "    moving_count = 0\n",
    "    static_count = 0\n",
    "    total_box_area = 0\n",
    "    current_positions = {}\n",
    "\n",
    "    for det in detections:\n",
    "        # Los resultados se almacenan como [x1, y1, x2, y2, conf, class_id]\n",
    "        x1, y1, x2, y2 = det.xyxy[0]\n",
    "        conf = det.conf[0]\n",
    "        class_id = det.cls[0]\n",
    "\n",
    "        if conf > confidence_threshold and class_id == 0:  # Clase 'person'\n",
    "            # Incrementar el conteo de personas\n",
    "            current_person_count += 1\n",
    "\n",
    "            # Dibujar las cajas de las personas detectadas\n",
    "            cv.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "            cv.putText(frame, f'Person: {conf:.2f}', (int(x1), int(y1) - 10),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Guardar la posición actual de la persona\n",
    "            current_positions[current_person_count] = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "\n",
    "            # Calcular el área ocupada por la caja\n",
    "            total_box_area += (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Actualizar valores solo si hubo detecciones\n",
    "    if current_person_count > 0:\n",
    "        person_count = current_person_count\n",
    "\n",
    "        # Calcular densidad de personas\n",
    "        frame_area = frame.shape[0] * frame.shape[1]\n",
    "        density = person_count / frame_area if frame_area > 0 else 0\n",
    "\n",
    "        # Determinar si las personas están en movimiento o estáticas\n",
    "        for person_id, current_pos in current_positions.items():\n",
    "            if person_id in previous_positions:\n",
    "                prev_pos = previous_positions[person_id]\n",
    "                distance = ((current_pos[0] - prev_pos[0]) ** 2 +\n",
    "                            (current_pos[1] - prev_pos[1]) ** 2) ** 0.5\n",
    "                if distance > 10:  # Umbral para considerar \"en movimiento\"\n",
    "                    moving_count += 1\n",
    "                else:\n",
    "                    static_count += 1\n",
    "            else:\n",
    "                static_count += 1\n",
    "\n",
    "        # Calcular proporción de movimiento\n",
    "        total_detected = moving_count + static_count\n",
    "        moving_ratio = moving_count / total_detected if total_detected > 0 else 0\n",
    "\n",
    "        # Calcular porcentaje de área ocupada por las cajas\n",
    "        occupied_area_ratio = total_box_area / frame_area if frame_area > 0 else 0\n",
    "\n",
    "    # Actualizar posiciones para el siguiente frame\n",
    "    previous_positions = current_positions\n",
    "\n",
    "    # Cambiar color del texto basado en el conteo de personas\n",
    "    text_color = (0, 255, 0) if person_count < 10 else (0, 0, 255)\n",
    "\n",
    "    # Mostrar indicadores en el frame\n",
    "    cv.putText(frame, f'Personas: {person_count}', (10, 40),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "    cv.putText(frame, f'Densidad: {density:.6f}', (10, 80),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv.putText(frame, f'Proporcion de Movimiento: {moving_ratio:.2f}', (10, 120),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv.putText(frame, f'Area Ocupada: {occupied_area_ratio:.2%}', (10, 160),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    # Redimensionar el frame antes de mostrarlo\n",
    "    frame_resized = cv.resize(frame, (1024, 576))\n",
    "\n",
    "    # Mostrar el frame con las detecciones\n",
    "    cv.imshow('Detección en Vivo', frame_resized)\n",
    "\n",
    "    # Mostrar el frame procesado\n",
    "    frame_resized = cv.resize(frame, (1024, 576))\n",
    "    cv.imshow('Detección en Vivo', frame_resized)\n",
    "\n",
    "    # Escribir el frame procesado en el archivo de video\n",
    "    out.write(frame_resized)\n",
    "\n",
    "    # Salir si se presiona la tecla 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Liberar recursos y cerrar la ventana\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
